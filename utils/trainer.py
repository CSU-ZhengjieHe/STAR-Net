import torch
import torch.nn as nn
import torch.nn.functional as F
from tqdm import tqdm
import logging
from pathlib import Path
import time
from collections import defaultdict

from .masking import DynamicMaskGenerator
from .losses import GANLoss, AdaptiveWeightScheduler, MultiScaleSpectralLoss

class Trainer:
    def __init__(self, config, generator, discriminator, train_loader, val_loader, device):
        self.config = config
        self.generator = generator
        self.discriminator = discriminator
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device
        self.metrics = {}
        self.best_val_loss = float('inf')
        
        # ğŸ†• æ·»åŠ åŠ¨æ€æ©ç ç”Ÿæˆå™¨
        self.mask_generator = DynamicMaskGenerator(config)
        
        # æ·»åŠ è‡ªé€‚åº”æƒé‡è°ƒåº¦å™¨
        self.weight_scheduler = AdaptiveWeightScheduler({
            'rec': config.lambda_rec,
            'spec': config.lambda_spec,
            'adv': config.lambda_adv,
            'gp': config.lambda_gp,
            'rel': config.lambda_rel
        }, beta=config.weight_beta)
        
        # åˆå§‹åŒ–æŸå¤±å‡½æ•°
        self.criterion = GANLoss(
            lambda_rec=config.lambda_rec,
            lambda_spec=config.lambda_spec,
            lambda_gp=config.lambda_gp
        ).to(device)
        
        # ä½¿ç”¨æ›´å¥½çš„ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        self.g_optimizer = torch.optim.AdamW(
            self.generator.parameters(),
            lr=config.lr_g,
            betas=(config.beta1, config.beta2),
            weight_decay=config.weight_decay
        )
        
        self.d_optimizer = torch.optim.AdamW(
            self.discriminator.parameters(),
            lr=config.lr_d,
            betas=(config.beta1, config.beta2),
            weight_decay=config.weight_decay
        )
        
        # ä½¿ç”¨ ReduceLROnPlateau è°ƒåº¦å™¨
        self.g_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.g_optimizer,
            mode='min',
            factor=0.5,
            patience=config.scheduler_patience,
            min_lr=config.min_lr
        )
        
        self.d_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.d_optimizer,
            mode='min',
            factor=0.5,
            patience=config.scheduler_patience,
            min_lr=config.min_lr
        )
        
        self.multi_scale_spec_loss = MultiScaleSpectralLoss()
        
        # æ·»åŠ æ—¶é—´è®°å½•ç›¸å…³å±æ€§
        self.train_start_time = None
        self.train_end_time = None
        self.total_training_time = None
        self.inference_times = []
        
        # è®¾ç½®æ—¥å¿—
        self.setup_logging()
    
    # ğŸ”„ ä¿®æ”¹create_dynamic_maskæ–¹æ³•
    def create_dynamic_mask(self, data, dynamic_mode=True):
        """
        åˆ›å»ºåŠ¨æ€ç¼ºå¤±æ©ç  - ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥è°ƒç”¨DynamicMaskGenerator
        """
        if dynamic_mode and self.config.dynamic_masking:
            return self.mask_generator.create_dynamic_mask(data)
        else:
            # å¦‚æœä¸ä½¿ç”¨åŠ¨æ€æ¨¡å¼ï¼Œè¿”å›å…¨1æ©ç ï¼ˆæ— ç¼ºå¤±ï¼‰
            return torch.ones_like(data)
    
    def calculate_d_loss(self, real_data, mask=None):
        """è®¡ç®—åˆ¤åˆ«å™¨çš„æŸå¤±"""
        batch_size = real_data.size(0)
        
        # ç”Ÿæˆå‡æ•°æ®
        with torch.no_grad():
            fake_data = self.generator(real_data, mask)
        
        # çœŸå®æ•°æ®çš„åˆ¤åˆ«ç»“æœ
        real_pred = self.discriminator(real_data)
        
        # ç”Ÿæˆæ•°æ®çš„åˆ¤åˆ«ç»“æœ
        fake_pred = self.discriminator(fake_data.detach())
        
        # WassersteinæŸå¤±
        d_loss = -torch.mean(real_pred) + torch.mean(fake_pred)
        
        # è®¡ç®—æ¢¯åº¦æƒ©ç½š
        alpha = torch.rand(batch_size, 1, 1, device=self.device)
        alpha = alpha.expand_as(real_data)
        
        # ç”Ÿæˆæ’å€¼æ ·æœ¬
        interpolated = alpha * real_data + (1 - alpha) * fake_data
        interpolated.requires_grad_(True)
        
        # è®¡ç®—æ’å€¼æ ·æœ¬çš„åˆ¤åˆ«ç»“æœ
        d_interpolated = self.discriminator(interpolated)
        
        # è®¡ç®—æ¢¯åº¦
        gradients = torch.autograd.grad(
            outputs=d_interpolated,
            inputs=interpolated,
            grad_outputs=torch.ones_like(d_interpolated),
            create_graph=True,
            retain_graph=True,
            only_inputs=True
        )[0]
        
        # ä¿®æ”¹è¿™é‡Œï¼šä½¿ç”¨reshapeè€Œä¸æ˜¯viewï¼Œå¹¶è®¡ç®—æ­£ç¡®çš„ç»´åº¦
        gradients = gradients.reshape(batch_size, -1)  # å°†æ‰€æœ‰ç»´åº¦å±•å¹³ä¸ºä¸€ç»´
        gradient_norm = gradients.norm(2, dim=1)  # è®¡ç®—L2èŒƒæ•°
        gradient_penalty = ((gradient_norm - 1) ** 2).mean()
        
        # æ€»æŸå¤±
        d_loss = d_loss + self.config.lambda_gp * gradient_penalty
        
        # è¿”å›æŸå¤±å’Œè¯¦ç»†ä¿¡æ¯
        return d_loss, {
            'd_loss_real': -torch.mean(real_pred).item(),
            'd_loss_fake': torch.mean(fake_pred).item(),
            'gradient_penalty': gradient_penalty.item(),
            'd_loss': d_loss.item()
        }
    
    def calculate_g_loss(self, real_data, fake_data, mask=None):
        # è®¡ç®—å„ä¸ªæŸå¤±
        fake_pred = self.discriminator(fake_data)
        g_loss_adv = -torch.mean(fake_pred)
        
        if mask is not None:
            g_loss_rec = F.mse_loss(
                fake_data * (1 - mask),
                real_data * (1 - mask)
            )
        else:
            g_loss_rec = F.mse_loss(fake_data, real_data)
        
        spec_loss = self.multi_scale_spec_loss(real_data, fake_data)
        
        # è·å–å½“å‰çš„è‡ªé€‚åº”æƒé‡
        current_weights = self.weight_scheduler.get_weights()
        
        # è®¡ç®—æ€»æŸå¤±
        g_loss = (
            current_weights['adv'] * g_loss_adv +
            current_weights['rec'] * g_loss_rec +
            current_weights['spec'] * spec_loss
        )
        
        # å‡†å¤‡å½“å‰æŸå¤±å­—å…¸ç”¨äºæ›´æ–°æƒé‡
        current_losses = {
            'adv': g_loss_adv.item(),
            'rec': g_loss_rec.item(),
            'spec': spec_loss.item()
        }
        
        # æ›´æ–°æƒé‡
        self.weight_scheduler.update(current_losses)
        
        return g_loss, {
            'g_loss_adv': g_loss_adv.item(),
            'g_loss_rec': g_loss_rec.item(),
            'spec_loss': spec_loss.item(),
            'weights': current_weights
        }
    
    def evaluate_metrics(self, real_data, generated_data, mask):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        with torch.no_grad():
            # åªè®¡ç®—ç¼ºå¤±éƒ¨åˆ†çš„æŒ‡æ ‡
            missing_mask = 1 - mask  # è½¬æ¢ä¸º1è¡¨ç¤ºç¼ºå¤±
            
            # æå–ç¼ºå¤±éƒ¨åˆ†çš„çœŸå®æ•°æ®å’Œç”Ÿæˆæ•°æ®
            real_missing = real_data * missing_mask
            gen_missing = generated_data * missing_mask
            
            # è®¡ç®—åŸºæœ¬æŒ‡æ ‡
            mse = F.mse_loss(gen_missing, real_missing).item()
            mae = F.l1_loss(gen_missing, real_missing).item()
            rmse = torch.sqrt(torch.tensor(mse)).item()
            
            # è®¡ç®—ç›¸å¯¹è¯¯å·® (é¿å…é™¤ä»¥0)
            denominator = torch.abs(real_missing) + 1e-6
            relative_error = torch.mean(
                torch.abs(gen_missing - real_missing) / denominator
            ).item()
            
            # è®¡ç®—é¢‘è°±è¯¯å·®
            real_fft = torch.fft.fft(real_data, dim=-1)
            gen_fft = torch.fft.fft(generated_data, dim=-1)
            spec_error = F.mse_loss(
                torch.abs(gen_fft), 
                torch.abs(real_fft)
            ).item()
            
            return {
                'mse': mse,
                'mae': mae,
                'rmse': rmse,
                'relative_error': relative_error,
                'spec_error': spec_error
            }
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Path(self.config.log_dir)
        log_dir.mkdir(parents=True, exist_ok=True)
        
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'training.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def train_epoch(self, epoch):
        self.generator.train()
        self.discriminator.train()
        
        total_g_loss = 0
        total_d_loss = 0
        epoch_g_losses = defaultdict(float)
        
        pbar = tqdm(self.train_loader, desc=f'Epoch {epoch}')
        
        for batch_idx, data in enumerate(pbar):
            # å¤„ç†æ•°æ®åŠ è½½å™¨è¿”å›çš„æ•°æ®
            if isinstance(data, (list, tuple)):
                real_data = data[0]
            else:
                real_data = data
                
            # ç¡®ä¿æ•°æ®æ˜¯å¼ é‡æ ¼å¼
            if not isinstance(real_data, torch.Tensor):
                real_data = torch.FloatTensor(real_data)
                
            real_data = real_data.to(self.device)
            
            # åˆ›å»ºåŠ¨æ€æ©ç 
            mask = self.create_dynamic_mask(real_data, dynamic_mode=True)
            mask = mask.to(self.device)
            
            # åº”ç”¨æ©ç åˆ°è¾“å…¥æ•°æ®
            masked_data = real_data * mask
            
            batch_size = real_data.size(0)
            
            # è®­ç»ƒåˆ¤åˆ«å™¨
            for _ in range(self.config.n_critic):
                self.d_optimizer.zero_grad()
                d_loss, d_loss_dict = self.calculate_d_loss(real_data, mask)
                d_loss.backward()
                # æ¢¯åº¦è£å‰ª
                d_grad_norm = torch.nn.utils.clip_grad_norm_(
                    self.discriminator.parameters(), 
                    self.config.gradient_clip
                )
                
                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸
                if torch.isnan(d_grad_norm) or torch.isinf(d_grad_norm):
                    self.logger.warning(f"Epoch {epoch}, Batch {batch_idx}: D gradient norm is {d_grad_norm}")
                    continue
                    
                self.d_optimizer.step()
            
            # ç”Ÿæˆå‡æ•°æ® - ä½¿ç”¨æ©ç æ•°æ®ä½œä¸ºè¾“å…¥
            fake_data = self.generator(masked_data, mask)
            
            # è®­ç»ƒç”Ÿæˆå™¨
            self.g_optimizer.zero_grad()
            g_loss, g_loss_dict = self.calculate_g_loss(real_data, fake_data, mask)
            g_loss.backward()
            # æ¢¯åº¦è£å‰ª
            g_grad_norm = torch.nn.utils.clip_grad_norm_(
                self.generator.parameters(), 
                self.config.gradient_clip
            )
            
            # æ¢¯åº¦æ£€æŸ¥
            if torch.isnan(g_grad_norm) or torch.isinf(g_grad_norm):
                self.logger.warning(f"Epoch {epoch}, Batch {batch_idx}: G gradient norm is {g_grad_norm}")
                continue
            self.g_optimizer.step()
            
            # æ›´æ–°æŸå¤±ç»Ÿè®¡
            total_g_loss += g_loss.item()
            total_d_loss += d_loss.item()
            
            # æ›´æ–°å„æŸå¤±é¡¹ç»Ÿè®¡
            for key, value in g_loss_dict.items():
                if key != 'weights' and isinstance(value, (float, int)):
                    epoch_g_losses[key] += value
                    
            # è·å–å½“å‰æƒé‡
            current_weights = g_loss_dict['weights']
            # è®¡ç®—å¹³å‡æŸå¤±
            avg_g_loss = total_g_loss / (batch_idx + 1)
            avg_d_loss = total_d_loss / (batch_idx + 1)
            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
            pbar.set_postfix({
                'D_loss': f'{avg_d_loss:.4f}',
                'G_loss': f'{avg_g_loss:.4f}',
                'D_grad': f'{d_grad_norm:.2f}',
                'G_grad': f'{g_grad_norm:.2f}',
                'rec_w': f'{current_weights["rec"]:.2f}',
                'spec_w': f'{current_weights["spec"]:.2f}',
                'adv_w': f'{current_weights["adv"]:.2f}'
            })
        
        # è®¡ç®—epochçº§åˆ«çš„å¹³å‡æŸå¤±
        avg_g_loss = total_g_loss / len(self.train_loader)
        avg_d_loss = total_d_loss / len(self.train_loader)
        
        # è®¡ç®—å„ä¸ªæŸå¤±é¡¹çš„å¹³å‡å€¼
        for key in epoch_g_losses:
            epoch_g_losses[key] /= len(self.train_loader)
        
        # éªŒè¯
        val_loss, metrics = self.validate()
        
        # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
        self.g_scheduler.step(val_loss)
        self.d_scheduler.step(val_loss)
        
        return avg_g_loss, avg_d_loss, val_loss, metrics
    
    @torch.no_grad()
    def validate(self):
        self.generator.eval()
        self.discriminator.eval()
        
        total_val_loss = 0
        all_metrics = defaultdict(float)
        num_batches = 0
        
        with torch.no_grad():
            for data in self.val_loader:
                if isinstance(data, (list, tuple)):
                    real_data = data[0]
                else:
                    real_data = data
                
                real_data = real_data.to(self.device)
                
                # åˆ›å»ºåŠ¨æ€æ©ç è¿›è¡ŒéªŒè¯
                mask = self.create_dynamic_mask(real_data, dynamic_mode=True)
                mask = mask.to(self.device)
                
                # åº”ç”¨æ©ç åˆ°è¾“å…¥
                masked_data = real_data * mask
                
                # ç”Ÿæˆæ¢å¤æ•°æ®
                fake_data = self.generator(masked_data, mask)
                
                # åªè¯„ä¼°è¢«æ©ç çš„éƒ¨åˆ†
                val_loss = F.mse_loss(fake_data * (1 - mask), real_data * (1 - mask))
                total_val_loss += val_loss.item()
                
                # è®¡ç®—å…¶ä»–æŒ‡æ ‡
                batch_metrics = self.evaluate_metrics(real_data, fake_data, mask)
                for key, value in batch_metrics.items():
                    all_metrics[key] += value
                
                num_batches += 1
        
        # è®¡ç®—å¹³å‡æŸå¤±å’ŒæŒ‡æ ‡
        avg_val_loss = total_val_loss / num_batches
        avg_metrics = {k: v / num_batches for k, v in all_metrics.items()}
        
        self.generator.train()
        self.discriminator.train()
        
        return avg_val_loss, avg_metrics
    
    def train(self):
        """è®­ç»ƒæ–¹æ³•"""
        self.train_start_time = time.time()  # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
        for epoch in range(self.config.num_epochs):
            # è·å–è®­ç»ƒç»“æœ
            train_g_loss, train_d_loss, val_loss, metrics = self.train_epoch(epoch)
            
            # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
            self.g_scheduler.step(val_loss)
            self.d_scheduler.step(val_loss)
            
            # è®°å½•å½“å‰epochçš„æŒ‡æ ‡
            current_metrics = {
                'train_g_loss': train_g_loss,
                'train_d_loss': train_d_loss,
                'val_loss': val_loss
            }
            # åˆå¹¶å…¶ä»–æŒ‡æ ‡
            if metrics:
                current_metrics.update(metrics)
            
            # ä¿å­˜åˆ°self.metrics
            self.metrics[epoch] = current_metrics
            
            # æ ¼å¼åŒ–æ—¥å¿—ä¿¡æ¯
            try:
                # åŸºç¡€è®­ç»ƒä¿¡æ¯
                log_msg = [
                    f'Epoch {epoch}',
                    f'G_loss={train_g_loss:.4f}',
                    f'D_loss={train_d_loss:.4f}',
                    f'Val_loss={val_loss:.4f}'
                ]
                
                # æ·»åŠ å…¶ä»–æŒ‡æ ‡
                metric_items = []
                for k, v in current_metrics.items():
                    if k not in ['train_g_loss', 'train_d_loss', 'val_loss']:
                        if isinstance(v, (float, int)):
                            metric_items.append(f'{k}={v:.4f}')
                        else:
                            metric_items.append(f'{k}={v}')
                
                if metric_items:
                    log_msg.append('Metrics: ' + ', '.join(metric_items))
                
                # åˆå¹¶æ‰€æœ‰æ—¥å¿—ä¿¡æ¯
                log_str = ' | '.join(log_msg)
                self.logger.info(log_str)
                
            except Exception as e:
                self.logger.error(f"Error formatting log message: {str(e)}")
                self.logger.error(f"Current metrics: {current_metrics}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_checkpoint(epoch, is_best=True)
                self.logger.info(f"New best model saved with val_loss={val_loss:.4f}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config.save_interval == 0:
                self.save_checkpoint(epoch)
                self.logger.info(f"Saved checkpoint for epoch {epoch}")
        
        self.train_end_time = time.time()  # è®°å½•è®­ç»ƒç»“æŸæ—¶é—´
        self.total_training_time = self.train_end_time - self.train_start_time
        
        # è®°å½•è®­ç»ƒæ—¶é—´ä¿¡æ¯
        hours, remainder = divmod(self.total_training_time, 3600)
        minutes, seconds = divmod(remainder, 60)
        self.logger.info(f"æ€»è®­ç»ƒæ—¶é—´: {int(hours)}å°æ—¶ {int(minutes)}åˆ†é’Ÿ {seconds:.2f}ç§’")
        return self.metrics
    
    def save_checkpoint(self, epoch, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'generator_state_dict': self.generator.state_dict(),
            'discriminator_state_dict': self.discriminator.state_dict(),
            'g_optimizer_state_dict': self.g_optimizer.state_dict(),
            'd_optimizer_state_dict': self.d_optimizer.state_dict(),
            'g_scheduler_state_dict': self.g_scheduler.state_dict(),
            'd_scheduler_state_dict': self.d_scheduler.state_dict(),
            'best_val_loss': self.best_val_loss,
            'metrics': self.metrics
        }
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹ç›®å½•
        checkpoint_dir = Path(self.config.checkpoint_dir)
        checkpoint_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹
        if not is_best:
            checkpoint_path = checkpoint_dir / f'checkpoint_epoch_{epoch}.pt'
            torch.save(checkpoint, checkpoint_path)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if is_best:
            best_model_path = checkpoint_dir / 'best_model.pt'
            torch.save(checkpoint, best_model_path)